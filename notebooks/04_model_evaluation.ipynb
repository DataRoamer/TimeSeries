{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation and Comparison\n",
    "\n",
    "This notebook provides comprehensive model evaluation techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(actual, predicted):\n",
    "    \"\"\"\n",
    "    Calculate comprehensive evaluation metrics\n",
    "    \"\"\"\n",
    "    mae = mean_absolute_error(actual, predicted)\n",
    "    mse = mean_squared_error(actual, predicted)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mape = np.mean(np.abs((actual - predicted) / actual)) * 100\n",
    "    r2 = r2_score(actual, predicted)\n",
    "    \n",
    "    # Additional metrics\n",
    "    median_ae = np.median(np.abs(actual - predicted))\n",
    "    max_error = np.max(np.abs(actual - predicted))\n",
    "    \n",
    "    return {\n",
    "        'MAE': mae,\n",
    "        'MSE': mse,\n",
    "        'RMSE': rmse,\n",
    "        'MAPE': mape,\n",
    "        'R²': r2,\n",
    "        'Median_AE': median_ae,\n",
    "        'Max_Error': max_error\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Residual Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_residuals(actual, predicted, model_name='Model'):\n",
    "    \"\"\"\n",
    "    Plot residual analysis\n",
    "    \"\"\"\n",
    "    residuals = actual - predicted\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Residuals vs Fitted\n",
    "    axes[0, 0].scatter(predicted, residuals, alpha=0.6)\n",
    "    axes[0, 0].axhline(y=0, color='red', linestyle='--')\n",
    "    axes[0, 0].set_xlabel('Predicted Values')\n",
    "    axes[0, 0].set_ylabel('Residuals')\n",
    "    axes[0, 0].set_title(f'{model_name} - Residuals vs Fitted')\n",
    "    \n",
    "    # Q-Q plot\n",
    "    stats.probplot(residuals, dist=\"norm\", plot=axes[0, 1])\n",
    "    axes[0, 1].set_title(f'{model_name} - Q-Q Plot')\n",
    "    \n",
    "    # Histogram of residuals\n",
    "    axes[1, 0].hist(residuals, bins=30, alpha=0.7, density=True)\n",
    "    axes[1, 0].set_xlabel('Residuals')\n",
    "    axes[1, 0].set_ylabel('Density')\n",
    "    axes[1, 0].set_title(f'{model_name} - Residual Distribution')\n",
    "    \n",
    "    # Time series of residuals (if index available)\n",
    "    if hasattr(actual, 'index'):\n",
    "        axes[1, 1].plot(actual.index, residuals)\n",
    "        axes[1, 1].axhline(y=0, color='red', linestyle='--')\n",
    "        axes[1, 1].set_xlabel('Time')\n",
    "        axes[1, 1].set_ylabel('Residuals')\n",
    "        axes[1, 1].set_title(f'{model_name} - Residuals Over Time')\n",
    "    else:\n",
    "        axes[1, 1].plot(residuals)\n",
    "        axes[1, 1].axhline(y=0, color='red', linestyle='--')\n",
    "        axes[1, 1].set_xlabel('Observation')\n",
    "        axes[1, 1].set_ylabel('Residuals')\n",
    "        axes[1, 1].set_title(f'{model_name} - Residuals Sequence')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Forecast Accuracy by Horizon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_by_horizon(actual, predicted, horizons=[1, 6, 12, 24]):\n",
    "    \"\"\"\n",
    "    Evaluate forecast accuracy at different horizons\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for h in horizons:\n",
    "        if h <= len(actual):\n",
    "            actual_h = actual[:h]\n",
    "            predicted_h = predicted[:h]\n",
    "            \n",
    "            metrics = calculate_metrics(actual_h, predicted_h)\n",
    "            metrics['Horizon'] = h\n",
    "            results.append(metrics)\n",
    "    \n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models(actual, predictions_dict):\n",
    "    \"\"\"\n",
    "    Compare multiple models\n",
    "    \"\"\"\n",
    "    comparison_results = []\n",
    "    \n",
    "    for model_name, predicted in predictions_dict.items():\n",
    "        metrics = calculate_metrics(actual, predicted)\n",
    "        metrics['Model'] = model_name\n",
    "        comparison_results.append(metrics)\n",
    "    \n",
    "    df_comparison = pd.DataFrame(comparison_results)\n",
    "    return df_comparison.set_index('Model')\n",
    "\n",
    "def plot_model_comparison(comparison_df, metric='MAE'):\n",
    "    \"\"\"\n",
    "    Plot model comparison for a specific metric\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    comparison_df[metric].plot(kind='bar')\n",
    "    plt.title(f'Model Comparison - {metric}')\n",
    "    plt.ylabel(metric)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Cross-Validation for Time Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_series_cv(data, model_func, n_splits=5, test_size=48):\n",
    "    \"\"\"\n",
    "    Time series cross-validation with expanding window\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    total_len = len(data)\n",
    "    \n",
    "    # Calculate split points\n",
    "    min_train_size = total_len - n_splits * test_size\n",
    "    \n",
    "    for i in range(n_splits):\n",
    "        train_end = min_train_size + i * test_size\n",
    "        test_start = train_end\n",
    "        test_end = test_start + test_size\n",
    "        \n",
    "        if test_end > total_len:\n",
    "            break\n",
    "            \n",
    "        train_data = data[:train_end]\n",
    "        test_data = data[test_start:test_end]\n",
    "        \n",
    "        # Train model and predict\n",
    "        predictions = model_func(train_data, len(test_data))\n",
    "        \n",
    "        # Calculate metrics\n",
    "        metrics = calculate_metrics(test_data, predictions)\n",
    "        metrics['Fold'] = i + 1\n",
    "        results.append(metrics)\n",
    "    \n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions_vs_actual(actual, predicted, model_name='Model'):\n",
    "    \"\"\"\n",
    "    Plot predicted vs actual values\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(actual, predicted, alpha=0.6)\n",
    "    \n",
    "    # Perfect prediction line\n",
    "    min_val = min(min(actual), min(predicted))\n",
    "    max_val = max(max(actual), max(predicted))\n",
    "    plt.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2)\n",
    "    \n",
    "    plt.xlabel('Actual Values')\n",
    "    plt.ylabel('Predicted Values')\n",
    "    plt.title(f'{model_name} - Predicted vs Actual')\n",
    "    \n",
    "    # Add R² score\n",
    "    r2 = r2_score(actual, predicted)\n",
    "    plt.text(0.1, 0.9, f'R² = {r2:.3f}', transform=plt.gca().transAxes, \n",
    "             bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_forecast_with_confidence(actual, predicted, confidence_intervals=None, model_name='Model'):\n",
    "    \"\"\"\n",
    "    Plot forecast with confidence intervals\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    \n",
    "    if hasattr(actual, 'index'):\n",
    "        time_index = actual.index\n",
    "    else:\n",
    "        time_index = range(len(actual))\n",
    "    \n",
    "    plt.plot(time_index, actual, label='Actual', color='black', linewidth=2)\n",
    "    plt.plot(time_index, predicted, label='Predicted', color='red', linestyle='--')\n",
    "    \n",
    "    if confidence_intervals is not None:\n",
    "        lower_ci, upper_ci = confidence_intervals\n",
    "        plt.fill_between(time_index, lower_ci, upper_ci, alpha=0.3, label='95% CI')\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.title(f'{model_name} - Forecast vs Actual')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Value')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}